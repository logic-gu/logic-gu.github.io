---
speaker: Tjeerd Fokkens
affil: FLoV
title: "Final Seminar: Cognitively Adequate Complexity of Description Logic"
date: 2025-05-23 10:15:00 +02:00
location: Humanisten, J233
event-type: Final Seminar

excerpt-only: false
---
__Opponent:__ Jakub Szymanik, Associate professor at the Center for Brain/Mind Sciences and the Computer Science Department at the University of Trento.

**Abstract**
Description logics are designed to reason efficiently with ontologies in knowledge bases, which are widely used in several industries.
Human errors commonly cause mistakes to occur in the axioms of these ontologies, which leads to the logics deducing false conclusions.
Ontology engineers spend much time on finding these false axioms to repair the ontology.
This process can be facilitated by presenting a justification of the false conclusion, i.e. a set of axioms that entail the conclusion.
Typically, the number of justifications for a given conclusion is very large and it is not clear which one should be presented.

<!--more-->

It is therefore useful to estimate the cognitively adequate complexity of description logic so that the simplest justification(s) can be identified and selected.
This work focusses on the cognitive complexity of deciding the consistency of ABoxes in the description logic $$\mathcal{ALE}$$.
Conceptual considerations entail that the complexity of the ABox consistency problem is expressed as a linear preorder on a large enough set of ABoxes.
This linear preorder can be estimated by an ACT-R model called SHARP.
The model is a tableau algorithm implemented in a cognitive architecture, thereby modelling a human brain executing the tableau algorithm, which forms an approximation of the cognitive complexity ordering.

A complexity ordering that is a linear preorder can be obtained from experimental data by aggregating the different individual orders from a sample into one so-called proto-order, in which the largest transitive subrelation is the desired linear pre-order.
The predicted complexity ordering by SHARP agrees well with the empirical data.

To enable applications in ontology debugging, a surrogate model is needed that accurately approximates SHARP's output with great computational efficiency.
Such a surrogate model can be obtained by the Random Forest algorithm trained on an appropriate dataset generated by SHARP.
The complexity ordering thus obtained agrees well with the one predicted by SHARP but can be estimated in a fraction of the time enabling it for practical applications.
